{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2687fa-fd01-45d0-a226-2479f1d26110",
   "metadata": {
    "id": "9d2687fa-fd01-45d0-a226-2479f1d26110"
   },
   "source": [
    "# COVID-19 Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pELm01Nz3vty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29282,
     "status": "ok",
     "timestamp": 1703407494075,
     "user": {
      "displayName": "Hassan",
      "userId": "12893018555795260021"
     },
     "user_tz": -300
    },
    "id": "pELm01Nz3vty",
    "outputId": "8c842a9c-03e0-4c86-afeb-b0c36942112d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Mount drive on colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pXR27jbi4w_m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1703407502294,
     "user": {
      "displayName": "Hassan",
      "userId": "12893018555795260021"
     },
     "user_tz": -300
    },
    "id": "pXR27jbi4w_m",
    "outputId": "7508f533-d8f5-4083-adf0-b4ee329521e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/COVID-19\n"
     ]
    }
   ],
   "source": [
    "#change directory to project folder\n",
    "%cd drive/MyDrive/COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ywg4_HtcFLh8",
   "metadata": {
    "id": "ywg4_HtcFLh8"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc8b55-cfaf-4eae-b152-af8406efb95d",
   "metadata": {
    "id": "d6fc8b55-cfaf-4eae-b152-af8406efb95d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from datetime import datetime\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Reduce tensorflow messages.\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715bcf7-27ea-4274-bbe3-9ad9894ba9a5",
   "metadata": {
    "id": "8715bcf7-27ea-4274-bbe3-9ad9894ba9a5"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67b5aa-df33-436b-9a54-7f61b826b3cb",
   "metadata": {
    "id": "9b67b5aa-df33-436b-9a54-7f61b826b3cb"
   },
   "outputs": [],
   "source": [
    "class cd:\n",
    "    # class to change current working directory for colab\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = os.path.expanduser(newPath)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0d31e-8c27-4292-9c8f-c8072785cffb",
   "metadata": {
    "id": "3fb0d31e-8c27-4292-9c8f-c8072785cffb"
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    #class to load data\n",
    "    def __init__(self):\n",
    "        self.covid_dir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series/\" #project directory\n",
    "        # Don't change the order of the items in this list.\n",
    "        self.files = [\"time_series_covid19_confirmed_global.csv\", \"time_series_covid19_deaths_global.csv\",\n",
    "                      \"time_series_covid19_recovered_global.csv\"] #files for the three timeseries\n",
    "        \n",
    "\n",
    "    def load_population_data(self):\n",
    "        #load population data from the kaggle dataset\n",
    "        pop = pd.read_csv(\"kaggle_data/covid19-global-forecasting-week-5/train.csv\")\n",
    "        # Remove unnecessary columns.\n",
    "        pop.drop(columns=[\"Id\", \"County\", \"Province_State\", \"Weight\", \"Date\", \"Target\", \"TargetValue\"], inplace=True)\n",
    "        # Find country's population\n",
    "        return pop.groupby(['Country_Region']).max()\n",
    "\n",
    "    def load_covid_data(self):\n",
    "        # load data from all the csv files\n",
    "        # download data from github repository if path doesn't exist\n",
    "        if not os.path.isdir(self.covid_dir):\n",
    "            subprocess.call([\"git\", \"clone\", \"https://github.com/CSSEGISandData/COVID-19.git\"])\n",
    "\n",
    "        # Create a list of paths from the file names.\n",
    "        paths = [os.path.join(self.covid_dir, f) for f in self.files]\n",
    "\n",
    "        # Get new data\n",
    "        if platform.system() == 'Linux':  # TODO make cross-platform.\n",
    "            stats = [os.stat(path) for path in paths]\n",
    "            # Get today's date.\n",
    "            today = datetime.utcfromtimestamp(int(time.time())).strftime('%Y-%m-%d')\n",
    "            # Get last modified date for each file.\n",
    "            dates = [datetime.utcfromtimestamp(int(stat.st_mtime)).strftime('%Y-%m-%d') for stat in stats]\n",
    "            # Check whether all data files have been modified today. If not get updates.\n",
    "            if not all(today == creation_time for creation_time in dates):\n",
    "                with cd(\"COVID-19\"):\n",
    "                    subprocess.call([\"git\", \"pull\", \"origin\", \"master\"])\n",
    "        else:\n",
    "            print(f\"Platform not supported: {platform.system()}\")\n",
    "            raise SystemExit\n",
    "\n",
    "        # Load all csv files into a list of data frames.\n",
    "        data_frames = [pd.read_csv(path) for path in paths]\n",
    "\n",
    "        # Remove unnecessary columns and return dataframes.\n",
    "        return [df.drop(columns=['Province/State', 'Lat', 'Long']) for df in data_frames]  # Confirmed, Dead and Recovered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2306e-429a-41a7-b481-350c8c2d1a4f",
   "metadata": {
    "id": "02f2306e-429a-41a7-b481-350c8c2d1a4f"
   },
   "source": [
    "## Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ebc32-0189-45f3-bee2-520bb69542a2",
   "metadata": {
    "id": "956ebc32-0189-45f3-bee2-520bb69542a2"
   },
   "outputs": [],
   "source": [
    "class Country:\n",
    "    \"\"\" Represents a country with COVID-19 \"\"\"\n",
    "\n",
    "    def __init__(self, n, p, c, d, r):\n",
    "        \"\"\"\n",
    "            Params:\n",
    "                n - string - The country's name.\n",
    "                p - int - The country's population.\n",
    "                c - pandas Series - Time series of the confirmed cases.\n",
    "                d - pandas Series - Time series of the deseased cases.\n",
    "                r - pandas Series - Time series of the recovered cases.\n",
    "        \"\"\"\n",
    "        self.name = n\n",
    "        self.encoded_name = None\n",
    "        self.population = p\n",
    "        self.data = pd.concat([c, d, r], axis=1)\n",
    "        self.data.columns=[\"Confirmed\", \"Deceased\", \"Recovered\"]\n",
    "\n",
    "    def find_divisor_offset(self, div):\n",
    "        \"\"\"\n",
    "        Calculates an offset to make the data divisible by the horizon.\n",
    "        By finding the greatest divisor of div that is smaller than num.\n",
    "\n",
    "        div  - int - horizon\n",
    "        returns offset - int\n",
    "\n",
    "        Example: If splitting the data into weeks.\n",
    "        num = 137\n",
    "        div = 7\n",
    "        137/7 = 19.57... (Not divisible)\n",
    "        Using the formula below:\n",
    "        137 - (floor(137/7) * 7) = 4\n",
    "        Four is the offset to make 137 divisible by 7 because\n",
    "        137 - 4 = 133\n",
    "        133/7 = 19 (Divisible)\n",
    "        \"\"\"\n",
    "        # Find how many days have to be cut of the start to make the data divisible by the horizon.\n",
    "        # This is required as the cross validation folds must the the same size.\n",
    "        time_steps = len(self.data)\n",
    "        return time_steps - ((time_steps//div)*div)\n",
    "\n",
    "    def split_k_fold(self, train_size, horizon):\n",
    "        \"\"\"\n",
    "        train_size - int - number of time steps in the train data.\n",
    "        horizon - int - number of time steps to be forecasted.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make the data divisible by the horizon to guarantee all k folds have the same size.\n",
    "        # Cuts out the old data instead of the new time steps.\n",
    "        offset = self.find_divisor_offset(horizon)\n",
    "        temp_data = self.data[offset:]\n",
    "\n",
    "        val_end = train_size + horizon\n",
    "        test_end = train_size + horizon * 2\n",
    "\n",
    "        train = temp_data[:train_size]\n",
    "        val = temp_data[train_size:val_end]\n",
    "        test_x = temp_data[:val_end]\n",
    "        test_y = temp_data[val_end:test_end]\n",
    "\n",
    "        return train, val, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c818c-9db2-41eb-9031-1edf08f5c675",
   "metadata": {
    "id": "0f7c818c-9db2-41eb-9031-1edf08f5c675"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44ca09-db0c-45fb-9456-ede3e750b7fa",
   "metadata": {
    "id": "dc44ca09-db0c-45fb-9456-ede3e750b7fa"
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\" A container that stores the time series COVID-19 data for each country.\n",
    "        The data is loaded from the data_loader module.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loader = DataLoader()\n",
    "        self.raw_confirmed, self.raw_deceased, self.raw_recovered = self.loader.load_covid_data()\n",
    "        self.population = self.loader.load_population_data()\n",
    "        self.countries = []\n",
    "        self.save_dir = \"cross_val_results\"\n",
    "        self.pad_val = -10000\n",
    "        self.horizon = 28\n",
    "        self.features = 3\n",
    "        self.padding = np.full((self.horizon, self.features), self.pad_val)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.populate_countries()\n",
    "        self.encode_names()\n",
    "        self.split_data()\n",
    "        self.save_data()\n",
    "\n",
    "        scaled_train, _ = self.standardize(self.train)\n",
    "        scaled_val, self.val_scalers = self.standardize(self.val)\n",
    "        scaled_test_x, _ = self.standardize(self.test_x)\n",
    "        scaled_test_y, self.test_y_scalers = self.standardize(self.test_y)\n",
    "\n",
    "        self.padded_scaled_train = self.pad_data(scaled_train)\n",
    "        self.padded_scaled_test_x = self.pad_data(scaled_test_x, offset=1)\n",
    "\n",
    "        self.multi_out_scaled_val = self.prepare_output_data(scaled_val)\n",
    "        self.multi_out_scaled_test_y = self.prepare_output_data(scaled_test_y)\n",
    "\n",
    "    def populate_countries(self):\n",
    "        \"\"\"\n",
    "            Creates a list of Country objects each containing the COVID-19 data for their respective country.\n",
    "            Retunrs:\n",
    "                A list of Country objects.\n",
    "        \"\"\"\n",
    "        # For each country in population.\n",
    "        for name, pop in self.population.iterrows():\n",
    "            p = pop['Population']\n",
    "            # Get all relevant time series based on country name.\n",
    "            c = self.raw_confirmed.loc[self.raw_confirmed['Country/Region'] == name].sum(numeric_only=True)\n",
    "            d = self.raw_deceased.loc[self.raw_deceased['Country/Region'] == name].sum(numeric_only=True)\n",
    "            r = self.raw_recovered.loc[self.raw_recovered['Country/Region'] == name].sum(numeric_only=True)\n",
    "            # Create new country object.\n",
    "            self.countries.append(Country(name, p, c, d, r))\n",
    "\n",
    "    def encode_names(self, output_space=6):\n",
    "        \"\"\"\n",
    "        Creates hashed versions of the country names.\n",
    "        These will be used in the Embedding layer of the network.\n",
    "        Param: output_space - int - size of the poutput produced by the hash functions to guarantee uniqueness.\n",
    "        Returns:\n",
    "        hashed_names - list - a list containing all the hashed names for the countries.\n",
    "        \"\"\"\n",
    "        # Hash all country names using SHA-256 then convert the hex output to int slice of the first digits by converting it to\n",
    "        # a string and save it as an int in a list.\n",
    "        hashed_names = [int(str(int(hashlib.sha256(country.encode('utf-8')).hexdigest(), 16))[:output_space]) for country in\n",
    "                        self.population.index]\n",
    "\n",
    "        # Convert the integers into an array of digits.\n",
    "        hashed_names = np.stack([np.array(list(map(int,str(x)))) for x in hashed_names])\n",
    "\n",
    "        for country, hashed_name in zip(self.countries, hashed_names):\n",
    "            country.encoded_name = hashed_name\n",
    "\n",
    "        self.encoded_names = hashed_names\n",
    "\n",
    "    def cross_validate(self, train_size):\n",
    "        \"\"\"\n",
    "        Cross validate data for all countries.\n",
    "        \"\"\"\n",
    "        train, val, test_x, test_y = [], [], [], []\n",
    "        for country in self.countries:\n",
    "            tr, v, te_x, te_y = country.split_k_fold(train_size, self.horizon)\n",
    "            train.append(tr), val.append(v), test_x.append(te_x), test_y.append(te_y)\n",
    "        return np.stack(train), np.stack(val), np.stack(test_x), np.stack(test_y)\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Returns four lists of numpy arrrays. Each list has the shape (fold, countries, days). The number of days changes over the\n",
    "        folds.\n",
    "        \"\"\"\n",
    "        self.train, self.val, self.test_x, self.test_y = [], [], [], []\n",
    "        train_size = self.horizon\n",
    "        # This assumes all countries have the same length.\n",
    "        # The minus two gives space for the validation and test sets as they will overshoot.\n",
    "        k_folds = len(self.countries[0].data)//self.horizon - 2\n",
    "        for _ in range(k_folds):\n",
    "            tr, v, te_x, te_y = self.cross_validate(train_size)\n",
    "            self.train.append(tr), self.val.append(v), self.test_x.append(te_x), self.test_y.append(te_y)\n",
    "            train_size += self.horizon\n",
    "\n",
    "    def save_to_npz_folds(self, data, file_name):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            data - list containing numpy arrays with length = folds.\n",
    "            file_name - string.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.save_dir, file_name)\n",
    "        with open(file_path, \"w+b\") as new_file:\n",
    "            np.savez(new_file, *data)\n",
    "\n",
    "    def save_data(self):\n",
    "        #save data to npz file\n",
    "        self.save_to_npz_folds(self.train, \"train\")\n",
    "        self.save_to_npz_folds(self.val, \"val\")\n",
    "        self.save_to_npz_folds(self.test_x, \"test_x\")\n",
    "        self.save_to_npz_folds(self.test_y, \"test_y\")\n",
    "\n",
    "    def standardize(self, data):\n",
    "        #scale data using Standard Scaler\n",
    "        scaled_data = []\n",
    "        scalers = []\n",
    "        for fold in data:\n",
    "            scaled_fold = []\n",
    "            scalers_fold = []\n",
    "            for country in fold:\n",
    "                scaler = StandardScaler()\n",
    "                scaled_fold.append(scaler.fit_transform(country))\n",
    "                scalers_fold.append(scaler)\n",
    "            scaled_data.append(np.stack(scaled_fold))\n",
    "            scalers.append(scalers_fold)\n",
    "        return scaled_data, scalers\n",
    "\n",
    "    def pad_data(self, data, offset=0):\n",
    "        #pad data\n",
    "        padded_scaled_data = []\n",
    "        folds = len(data) - offset\n",
    "        for fold in data:\n",
    "            padded_fold = []\n",
    "            fold_padding = np.repeat(self.padding, folds).reshape(self.horizon*folds, self.features)\n",
    "            for row in fold:\n",
    "                padded_fold.append(np.append(row, fold_padding, axis=0))\n",
    "            folds -= 1\n",
    "            padded_scaled_data.append(np.stack(padded_fold))\n",
    "        return padded_scaled_data\n",
    "\n",
    "    def make_multi_output_data(self, data):\n",
    "        \"\"\"\n",
    "        Takes a dataset with shape (sample, timestep, feature) and\n",
    "        reshapes it to (feature, sample, timestep)\n",
    "        Returns: 3D numpy array\n",
    "        \"\"\"\n",
    "        confirmed, deceased, recovered = [], [], []\n",
    "        for sample in data:\n",
    "            confirmed.append(sample[:,0])\n",
    "            deceased.append(sample[:,1])\n",
    "            recovered.append(sample[:,2])\n",
    "        confirmed = np.stack(confirmed)\n",
    "        deceased = np.stack(deceased)\n",
    "        recovered = np.stack(recovered)\n",
    "        return np.stack([confirmed, deceased, recovered])\n",
    "\n",
    "    def prepare_output_data(self, data):\n",
    "        multi_out_data = []\n",
    "        for fold in data:\n",
    "            multi_out_data.append(self.make_multi_output_data(fold))\n",
    "        return multi_out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782c46a-eccb-43c4-803d-9231324ed831",
   "metadata": {
    "id": "c782c46a-eccb-43c4-803d-9231324ed831"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cff4e-cedd-4dea-b032-18943a99410d",
   "metadata": {
    "id": "157cff4e-cedd-4dea-b032-18943a99410d"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder block that takes as input a time series and a numerial representation of a county name\n",
    "    and creates a learned representation to be processed further in the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_units, rnn_layer, rnn_activation, pad_val, l1=0, l2=0, dropout=0):\n",
    "        super().__init__()\n",
    "        regularizer = tf.keras.regularizers.L1L2(l1=l1, l2=l2)\n",
    "\n",
    "        self.mask_layer = None\n",
    "        if pad_val:\n",
    "            self.mask_layer = layers.Masking(mask_value=pad_val)\n",
    "        #defined encoder architecture\n",
    "        self.hidden_rnn = rnn_layer(rnn_units, activation=rnn_activation, kernel_regularizer=regularizer, dropout=dropout,\n",
    "                                    name=\"rnn_encoder\")\n",
    "        self.hidden_dense = layers.Dense(1, name=\"name_encoder\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.mask_layer:\n",
    "            masked_inputs = self.mask_layer(inputs[0])\n",
    "        #call the architecture\n",
    "        h_rnn = self.hidden_rnn(masked_inputs)\n",
    "        h_dense = self.hidden_dense(inputs[1])\n",
    "\n",
    "        return layers.concatenate([h_rnn, h_dense], name=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417fd1d9-8b69-4ea1-9712-a3b01439715a",
   "metadata": {
    "id": "417fd1d9-8b69-4ea1-9712-a3b01439715a"
   },
   "outputs": [],
   "source": [
    "class MultiOutputRNN(keras.Model):\n",
    "    \"\"\"\n",
    "    Multi output RNN model with individual weights on the output nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, rnn_units, rnn_layer, rnn_activation, l1, l2, dropout, pad_val=None):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderBlock(rnn_units, rnn_layer, rnn_activation, pad_val, l1, l2, dropout)\n",
    "        #decoder block\n",
    "        self.c_out = layers.Dense(output_size, name=\"confirmed\")\n",
    "        self.d_out = layers.Dense(output_size, name=\"deceased\")\n",
    "        self.r_out = layers.Dense(output_size, name=\"recovered\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context = self.encoder(inputs)\n",
    "        return self.c_out(context), self.d_out(context), self.r_out(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590e081-9c52-4df0-8e77-fb34c08f5763",
   "metadata": {
    "id": "7590e081-9c52-4df0-8e77-fb34c08f5763"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, output_size, rnn_units, rnn_layer, rnn_activation, l1, l2, dropout, pad_val):\n",
    "        #set model according to parameters passed\n",
    "        self.model = MultiOutputRNN(output_size, rnn_units, rnn_layer, rnn_activation, l1, l2, dropout, pad_val)\n",
    "    #compile model\n",
    "    def compile(self, optimizer, loss, metrics):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    #fit model\n",
    "    def fit(self, x, y, epochs, callbacks=None, verbose=2):\n",
    "        return self.model.fit(x=x, y=y, epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
    "    #evaluate model\n",
    "    def evaluate(self, x, y, verbose=1, return_dict=True):\n",
    "        return self.model.evaluate(x=x, y=y, verbose=verbose, return_dict=return_dict)\n",
    "    #test model\n",
    "    def predict(self, inputs):\n",
    "        return self.model.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5df55-8279-49a6-af42-8d3c907bcc74",
   "metadata": {
    "id": "07a5df55-8279-49a6-af42-8d3c907bcc74"
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd104f6c-ce8c-49f9-b08c-da3009e42fd6",
   "metadata": {
    "id": "fd104f6c-ce8c-49f9-b08c-da3009e42fd6"
   },
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    #main class where all the experiments are performed\n",
    "    def __init__(self, val_scalers, test_scalers):\n",
    "        self.logger = logging.getLogger(\"experiment\") #log process\n",
    "        self.save_dir = \"cross_val_results\" #save directory\n",
    "        self.ensemble_size = 1\n",
    "        self.units = 20 #number of units\n",
    "        self.epochs = 300 #number of epochs\n",
    "        self.val_scalers = val_scalers \n",
    "        self.test_scalers = test_scalers\n",
    "        # layers and functions defines\n",
    "        self.tanh = tf.keras.activations.tanh\n",
    "        self.lstm = tf.keras.layers.LSTM\n",
    "        self.gru = tf.keras.layers.GRU\n",
    "        self.adam = tf.keras.optimizers.legacy.Adam()\n",
    "        self.mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "        self.metrics = [tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.RootMeanSquaredError()]\n",
    "        # Regularization methods according to table in research paper\n",
    "        self.regularizers = [[0,    0,    0],    # No regularization\n",
    "                             [0.01, 0,    0],    # Default L1 only.\n",
    "                             [0,    0.01, 0],    # Default L2 only.\n",
    "                             [0,    0,    0.2],  # Small dropout only.\n",
    "                             [0.01, 0.01, 0],    # Default L1 and L2, no dropout.\n",
    "                             [0.01, 0.01, 0.2]   # All regularizers.\n",
    "                            ]\n",
    "\n",
    "    def destandardize_data(self, data, scalers):\n",
    "        \"\"\"\n",
    "        data - numpy array of shape (countries, horizon, features)\n",
    "        returns numpy array of shape shape as data\n",
    "        \"\"\"\n",
    "        rescaled_data = []\n",
    "        for country_pred, scaler in zip(data, scalers):\n",
    "            rescaled_data.append(scaler.inverse_transform(country_pred))\n",
    "        return np.stack(rescaled_data)\n",
    "\n",
    "    def prepare_predictions(self, data, test_scalers):\n",
    "        \"\"\"\n",
    "        data - list - contains one numpy array of shape (countries, horizon) for each feature.\n",
    "        returns numpy array of shape (countries, horizon, features) with rescaled data.\n",
    "        \"\"\"\n",
    "        # First reshape the data.\n",
    "        reshaped_preds = []\n",
    "        # Unpack all sub lists into the three variables.\n",
    "        for c, d, r in zip(*data):\n",
    "            reshaped_preds.append(np.stack([c, d, r]).T)\n",
    "        reshaped_preds = np.stack(reshaped_preds)\n",
    "\n",
    "        return self.destandardize_data(reshaped_preds, test_scalers)\n",
    "\n",
    "    def calculate_rmse(self, orig, pred):\n",
    "        \"\"\"\n",
    "        orig - numpy array of shape (countries, horizon, features)\n",
    "        pred - numpy array of shape (countries, horizon, features)\n",
    "        Returns numpy array of shape (countries, features) containing the RMSE of all predictions\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for o, p in zip(orig, pred):\n",
    "            results.append(mean_squared_error(o, p, multioutput='raw_values', squared=False))\n",
    "        return np.stack(results)\n",
    "\n",
    "    def calculate_mae(self, orig, pred):\n",
    "        \"\"\"\n",
    "        orig - numpy array of shape (countries, horizon, features)\n",
    "        pred - numpy array of shape (countries, horizon, features)\n",
    "        Returns numpy array of shape (countries, features) containing the MAE of all predictions\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for o, p in zip(orig, pred):\n",
    "            results.append(mean_absolute_error(o, p, multioutput='raw_values'))\n",
    "        return np.stack(results)\n",
    "\n",
    "    def save_to_npz(self, data, file_name, exp_name):\n",
    "        #save npz file\n",
    "        file_path = os.path.join(self.save_dir, f\"{file_name}_{exp_name}\")\n",
    "        with open(file_path, \"w+b\") as new_file:\n",
    "            np.savez(new_file, data)\n",
    "\n",
    "    def run_experiments(self, horizon, padding, train, val, test_x, test_y, orig_val, orig_test_y, enc_names):\n",
    "        #run experiments mentioned in paper\n",
    "        fold_idx = 0\n",
    "        data_zip = zip(train, val, test_x, test_y)\n",
    "        for tr, v, te_x, te_y in data_zip:\n",
    "            for reg_idx, regularizer in enumerate(self.regularizers):\n",
    "                for trial in range(self.ensemble_size):\n",
    "                    #defined lstm model\n",
    "                    lstm = Model(horizon, self.units, self.lstm, self.tanh, regularizer[0], regularizer[1], regularizer[2],\n",
    "                                       padding)\n",
    "                    #defined gru model\n",
    "                    gru = Model(horizon, self.units, self.gru, self.tanh, regularizer[0], regularizer[1], regularizer[2],\n",
    "                                      padding)\n",
    "\n",
    "                    #compiled both models\n",
    "                    lstm.compile(self.adam, self.mse_loss, self.metrics)\n",
    "                    gru.compile(self.adam, self.mse_loss, self.metrics)\n",
    "\n",
    "                    #fit both models and log the process\n",
    "                    self.logger.info(f\"Training LSTM fold {fold_idx} regularizer {reg_idx} ensemble {trial}\")\n",
    "                    lstm_hist = lstm.fit([tr, enc_names], [v[0], v[1], v[2]], self.epochs, verbose=0)\n",
    "                    self.logger.info(f\"Training GRU fold {fold_idx} regularizer {reg_idx} ensemble {trial}\")\n",
    "                    gru_hist = gru.fit([tr, enc_names], [v[0], v[1], v[2]], self.epochs, verbose=0)\n",
    "\n",
    "                    # make predictions on validation data\n",
    "                    lstm_pred_val = lstm.predict([tr, enc_names])\n",
    "                    gru_pred_val = gru.predict([tr, enc_names])\n",
    "\n",
    "                    # make predictions on test data\n",
    "                    lstm_pred_test = lstm.predict([te_x, enc_names])\n",
    "                    gru_pred_test = gru.predict([te_x, enc_names])\n",
    "\n",
    "                    # rescale validation predictions\n",
    "                    lstm_pred_val = self.prepare_predictions(lstm_pred_val, self.val_scalers[fold_idx])\n",
    "                    gru_pred_val = self.prepare_predictions(gru_pred_val, self.val_scalers[fold_idx])\n",
    "\n",
    "                    # rescale test predictions\n",
    "                    lstm_pred_test = self.prepare_predictions(lstm_pred_test, self.test_scalers[fold_idx])\n",
    "                    gru_pred_test = self.prepare_predictions(gru_pred_test, self.test_scalers[fold_idx])\n",
    "\n",
    "                    # calculate RMSE for validation\n",
    "                    lstm_rmse_val = self.calculate_rmse(orig_val[fold_idx], lstm_pred_val)\n",
    "                    gru_rmse_val = self.calculate_rmse(orig_val[fold_idx], gru_pred_val)\n",
    "\n",
    "                    # calculate RMSE for test\n",
    "                    lstm_rmse_test = self.calculate_rmse(orig_test_y[fold_idx], lstm_pred_test)\n",
    "                    gru_rmse_test = self.calculate_rmse(orig_test_y[fold_idx], gru_pred_test)\n",
    "\n",
    "                    # calculate MAE for validation\n",
    "                    lstm_mae_val = self.calculate_mae(orig_val[fold_idx], lstm_pred_val)\n",
    "                    gru_mae_val = self.calculate_mae(orig_val[fold_idx], gru_pred_val)\n",
    "\n",
    "                    # calculate MAE for test\n",
    "                    lstm_mae_test = self.calculate_mae(orig_test_y[fold_idx], lstm_pred_test)\n",
    "                    gru_mae_test = self.calculate_mae(orig_test_y[fold_idx], gru_pred_test)\n",
    "\n",
    "                    lstm_name = f\"fold_{fold_idx}_reg_{reg_idx}_ens_{trial}_lstm\"\n",
    "                    gru_name = f\"fold_{fold_idx}_reg_{reg_idx}_ens_{trial}_gru\"\n",
    "\n",
    "                    # save model's predictions on validation data\n",
    "                    self.save_to_npz(lstm_pred_val, lstm_name, \"val_pred\")\n",
    "                    self.save_to_npz(gru_pred_val, gru_name, \"val_pred\")\n",
    "\n",
    "                    # save model's predictions on test data\n",
    "                    self.save_to_npz(lstm_pred_test, lstm_name, \"test_pred\")\n",
    "                    self.save_to_npz(gru_pred_test, gru_name, \"test_pred\")\n",
    "\n",
    "                    # save model's error scores for validation\n",
    "                    self.save_to_npz(lstm_rmse_val, lstm_name, \"rmse_val\")\n",
    "                    self.save_to_npz(gru_rmse_val, gru_name, \"rmse_val\")\n",
    "\n",
    "                    self.save_to_npz(lstm_mae_val, lstm_name, \"mae_val\")\n",
    "                    self.save_to_npz(gru_mae_val, gru_name, \"mae_val\")\n",
    "\n",
    "                    # save model's error scores for test\n",
    "                    self.save_to_npz(lstm_rmse_test, lstm_name, \"rmse_test\")\n",
    "                    self.save_to_npz(gru_rmse_test, gru_name, \"rmse_test\")\n",
    "\n",
    "                    self.save_to_npz(lstm_mae_test, lstm_name, \"mae_test\")\n",
    "                    self.save_to_npz(gru_mae_test, gru_name, \"mae_test\")\n",
    "            fold_idx += 1\n",
    "            if fold_idx == 1:\n",
    "              break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41f010-a65f-4e8e-a68a-a9c7b553ba2d",
   "metadata": {
    "id": "2f41f010-a65f-4e8e-a68a-a9c7b553ba2d"
   },
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae024b2-ac0a-49f0-9f33-001831960ae4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1702737,
     "status": "ok",
     "timestamp": 1703410212904,
     "user": {
      "displayName": "Hassan",
      "userId": "12893018555795260021"
     },
     "user_tz": -300
    },
    "id": "0ae024b2-ac0a-49f0-9f33-001831960ae4",
    "outputId": "22299dfa-93a7-4f03-816e-f33878c9347b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 89ms/step\n",
      "6/6 [==============================] - 2s 76ms/step\n",
      "6/6 [==============================] - 1s 91ms/step\n",
      "6/6 [==============================] - 1s 84ms/step\n"
     ]
    }
   ],
   "source": [
    "# log results \n",
    "logging.basicConfig(filename=\"logger.log\", format=\"%(asctime)s %(name)s %(levelname)s: %(message)s\", level=logging.DEBUG)\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\"))\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "dat = Data()\n",
    "dat.prepare_data()\n",
    "\n",
    "logging.info(f\"Number of days in data: {len(dat.countries[0].data)}\")\n",
    "logging.info(f\"Current date and time: {datetime.now()}\")\n",
    "\n",
    "exp = Experiment(dat.val_scalers, dat.test_y_scalers)\n",
    "# run experiments defined in class above\n",
    "exp.run_experiments(dat.horizon, dat.pad_val, dat.padded_scaled_train, dat.multi_out_scaled_val, dat.padded_scaled_test_x,\n",
    "                    dat.multi_out_scaled_test_y, dat.val, dat.test_y, dat.encoded_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7iZDwpMaAxSZ",
   "metadata": {
    "id": "7iZDwpMaAxSZ"
   },
   "source": [
    "## Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VvfUhzUMBK29",
   "metadata": {
    "id": "VvfUhzUMBK29"
   },
   "outputs": [],
   "source": [
    "TARGET_DIR = \"cross_val_results\"\n",
    "\n",
    "# process the result files saved\n",
    "def save_country_names\n",
    "    # save country names\n",
    "    d = Data()\n",
    "    names = [country.name for country in d.countries]\n",
    "    path = os.path.join(TARGET_DIR, \"country_names.csv\")\n",
    "    with open(path, \"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(names) \n",
    "\n",
    "def load_country_names():\n",
    "    #load country names\n",
    "    path = os.path.join(TARGET_DIR, \"country_names.csv\")\n",
    "    with open(path, \"r\", newline=\"\") as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        results = [row for row in reader]\n",
    "    return results[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_country_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TYiIDga0BUPi",
   "metadata": {
    "id": "TYiIDga0BUPi"
   },
   "source": [
    "## Result Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ylufR-46BXcK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469966,
     "status": "ok",
     "timestamp": 1703411561317,
     "user": {
      "displayName": "Hassan",
      "userId": "12893018555795260021"
     },
     "user_tz": -300
    },
    "id": "ylufR-46BXcK",
    "outputId": "cdbc4833-3cd6-4f02-d17e-8ef129bd6eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrl}\n",
      "\\hline\n",
      "  319.5   &  446.625 &  363.091 &  333.801 &  427.622 &  471.972 & GRU  \\\\\n",
      "  359.726 &  401.704 &  340.661 &  311.689 &  421.056 &  595.859 & LSTM \\\\\n",
      "  280.821 &  272.085 &  263.255 &  332.316 &  266.776 &  287.116 & GRU  \\\\\n",
      "  287.663 &  284.511 &  292.158 &  288.097 &  276.452 &  351.49  & LSTM \\\\\n",
      "  345.437 &  306.365 &  304.89  &  327.988 &  309.39  &  315.74  & GRU  \\\\\n",
      "  338.742 &  354.014 &  348.156 &  331.267 &  406.766 &  344.074 & LSTM \\\\\n",
      "  500.506 &  500.684 &  498.038 &  505.672 &  507.41  &  516.519 & GRU  \\\\\n",
      "  518.53  &  523.206 &  540.011 &  499.124 &  523.93  &  524.885 & LSTM \\\\\n",
      "  595.099 &  559.33  &  583.883 &  578.939 &  564.906 &  567.642 & GRU  \\\\\n",
      "  589.371 &  557.974 &  593.289 &  585.837 &  569.296 &  577.883 & LSTM \\\\\n",
      "  540.783 &  554.767 &  554.285 &  548.472 &  561.092 &  531.013 & GRU  \\\\\n",
      "  544.32  &  538.935 &  540.229 &  545.547 &  560.693 &  526.886 & LSTM \\\\\n",
      "  556.193 &  581.371 &  574.816 &  567.757 &  581.383 &  566.127 & GRU  \\\\\n",
      "  606.545 &  612.036 &  573.418 &  585.505 &  592.708 &  585.544 & LSTM \\\\\n",
      " 1079.87  & 1102.78  & 1083.06  & 1051.72  & 1111.1   & 1100.19  & GRU  \\\\\n",
      " 1115.62  & 1127.57  & 1103.42  & 1075.26  & 1107.91  & 1102.34  & LSTM \\\\\n",
      " 1386.9   & 1372.74  & 1394.95  & 1383.56  & 1362.15  & 1289.82  & GRU  \\\\\n",
      " 1338.21  & 1249.41  & 1326.41  & 1249.04  & 1246.11  & 1228.13  & LSTM \\\\\n",
      " 8334.14  & 8222.74  & 8199.98  & 8358.38  & 8234.36  & 8175.56  & GRU  \\\\\n",
      " 8169.59  & 8119.58  & 8163.58  & 8145.85  & 8142.64  & 8136.99  & LSTM \\\\\n",
      " 1117.13  & 1168.49  & 1129.21  & 1147.51  & 1174.43  & 1139.43  & GRU  \\\\\n",
      " 1111.14  & 1141.02  & 1103.93  & 1108.95  & 1182.09  & 1127.74  & LSTM \\\\\n",
      "  782.422 &  813.302 &  774.916 &  812.585 &  841.687 &  831.227 & GRU  \\\\\n",
      "  766.471 &  790.124 &  761.521 &  750.403 &  788.268 &  768.902 & LSTM \\\\\n",
      " 1042.4   & 1058.91  & 1016.43  & 1037.51  & 1028.77  & 1035.94  & GRU  \\\\\n",
      "  988.429 & 1039.74  &  999.749 &  986.289 & 1050.66  & 1086.26  & LSTM \\\\\n",
      " 1867.77  & 1892.92  & 1846.63  & 1816.71  & 1933.84  & 1903.54  & GRU  \\\\\n",
      " 1764.08  & 1903.12  & 1718.07  & 1845.21  & 1889.1   & 1921.64  & LSTM \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# view results from saved files\n",
    "#build file names according to make, variant of model\n",
    "def build_file_name(fold, reg, ens, model, error, partition):\n",
    "    return f\"fold_{fold}_reg_{reg}_ens_{ens}_{model}_{error}_{partition}\"\n",
    "#load npz files\n",
    "def load_npz(file_name):\n",
    "    result_dir = \"cross_val_results\"\n",
    "\n",
    "    file_path = os.path.join(result_dir, file_name)\n",
    "    with np.load(file_path) as data:\n",
    "        ret = [data[i] for i in data]  # There can be multiple arrays in a file.\n",
    "    return np.squeeze(np.array(ret))\n",
    "#format errors if any\n",
    "def format_errors(fold, model, error, partition):\n",
    "    ensemble_size = 10\n",
    "    regularizers = 6\n",
    "    results = []\n",
    "    for regularizer in range(regularizers):\n",
    "        mean_error = []\n",
    "        for trial in range(ensemble_size):\n",
    "            file_name = build_file_name(fold, regularizer, trial, model, error, partition)\n",
    "            model_errors = load_npz(file_name)\n",
    "            mean_error.append(np.mean(model_errors))\n",
    "        results.append(mean_error)\n",
    "    return np.stack(results).T\n",
    "#draw box plots\n",
    "def make_box_plot(lstm, gru, fold):\n",
    "    def draw_plot(data, offset,edge_color, fill_color):\n",
    "        pos = np.arange(data.shape[1])+offset\n",
    "        bp = ax.boxplot(data, positions= pos, widths=0.3, showmeans=True, meanline=True, patch_artist=True, manage_ticks=False)\n",
    "        for element in ['boxes', 'whiskers', 'fliers', 'medians', 'caps', 'means']:\n",
    "            plt.setp(bp[element], color=edge_color)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set(facecolor=fill_color)\n",
    "        return bp\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bpA = draw_plot(lstm, -0.2, \"black\", \"white\")\n",
    "    bpB = draw_plot(gru, +0.2,\"grey\", \"white\")\n",
    "    plt.xticks(range(6))\n",
    "\n",
    "    ax.set_xticklabels([\"No reg\", \"L1\", \"L2\", \"Dropout\", \"ElasticNet\", \"All regs\"])\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(f\"Comparison of models for validation fold {fold}\")\n",
    "    ax.legend([bpA[\"boxes\"][0], bpB[\"boxes\"][0]], [\"LSTM\", \"GRU\"])\n",
    "\n",
    "    fig_path = os.path.join(\"plots\", f\"boxplot_{fold}\")\n",
    "    plt.savefig(fig_path)\n",
    "\n",
    "def make_comparison_table_old(gru, lstm, fold):\n",
    "    #make comparison table\n",
    "    header = [\"Model\", \"Type\", \"No reg\", \"L1\", \"L2\", \"Dropout\", \"ElasticNet\", \"All regs\"]\n",
    "    with open(\"model_comparison.csv\", \"a\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        gru_mean_row = [\"GRU\", \"Mean\", *list(np.mean(gru, axis=0))]\n",
    "        gru_var_row = [\"GRU\", \"Var\", *list(np.var(gru, axis=0))]\n",
    "        lstm_mean_row = [\"LSTM\", \"Mean\", *list(np.mean(lstm, axis=0))]\n",
    "        lstm_var_row = [\"LSTM\", \"Var\", *list(np.var(lstm, axis=0))]\n",
    "        writer.writerow(gru_mean_row)\n",
    "        writer.writerow(gru_var_row)\n",
    "        writer.writerow(lstm_mean_row)\n",
    "        writer.writerow(lstm_var_row)\n",
    "\n",
    "def make_box_plots():\n",
    "    #call the box plot function for a model\n",
    "    folds = 14\n",
    "    for fold in range(folds):\n",
    "        gru_errors = format_errors(fold, \"gru\", \"rmse\", \"test\")\n",
    "        lstm_errors = format_errors(fold, \"lstm\", \"rmse\", \"test\")\n",
    "        make_box_plot(lstm_errors, gru_errors, fold)\n",
    "        make_comparison_table_old(lstm_errors, gru_errors, fold)\n",
    "\n",
    "def load_location_names():\n",
    "    #load location names from results\n",
    "    path = os.path.join(\"cross_val_results\", \"country_names.csv\")\n",
    "    with open(path, \"r\", newline=\"\") as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        results = [row for row in reader]\n",
    "    return results[0]\n",
    "\n",
    "def make_ensemble_plots(fold, reg, model, partition):\n",
    "    #ensemble the plots\n",
    "    preds = []\n",
    "    errs = []\n",
    "    ensemble_size = 10\n",
    "    loc_names = load_location_names()\n",
    "    display = (0, 10, 11)\n",
    "    sub_titles = [\"Confirmed\", \"Deceased\", \"Recovered\"]\n",
    "    orig = load_npz(\"test_y\")\n",
    "\n",
    "    for e in range(ensemble_size):\n",
    "        pred_file_name = build_file_name(fold, reg, e, model, partition, \"pred\")\n",
    "        error_file_name = build_file_name(fold, reg, e, model, \"rmse\", partition)\n",
    "        preds.append(load_npz(pred_file_name))\n",
    "        errs.append(load_npz(error_file_name))\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    avg_errs = np.mean(errs, axis=0)\n",
    "\n",
    "    fig, axes = plt.subplots(1,3, figsize=(8,6))\n",
    "    for loc_idx, loc_name in enumerate(loc_names):\n",
    "        for idx, axis in enumerate(axes):\n",
    "            axis.cla()\n",
    "            for p in preds:\n",
    "                axis.plot(p[loc_idx].T[idx], color=\"lightgrey\", label=\"Ensemble\")\n",
    "            axis.plot(orig[fold][loc_idx].T[idx], color=\"black\", label=\"Original data\")\n",
    "            axis.plot(avg_pred[loc_idx].T[idx], linestyle=\"--\", color=\"black\", label=f\"Average {model.upper()}\")\n",
    "            axis.set_title(f\"{sub_titles[idx]} : {round(avg_errs[loc_idx][idx], 3)} RMSE\", pad=12)\n",
    "        handles, labels = axes[2].get_legend_handles_labels()\n",
    "        axes[0].set_ylabel(\"People\")\n",
    "        axes[1].set_xlabel(\"Days\")\n",
    "        axes[2].legend([handle for i, handle in enumerate(handles) if i in display],\n",
    "                       [label for i, label in enumerate(labels) if i in display], loc=0)\n",
    "        fig.suptitle(loc_name)\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(\"plots\", f\"fold_{fold}_reg_{reg}_{model}_{partition}_{loc_name}\")\n",
    "        plt.savefig(fig_path, bbox_inches=\"tight\")\n",
    "\n",
    "def make_comparison_table(error_metric):\n",
    "    # call comparison table function\n",
    "    folds = 14\n",
    "    regs = 6\n",
    "    ens = 10\n",
    "    err_table = []\n",
    "    for fold in range(folds):\n",
    "        gru_err_row = []\n",
    "        lstm_err_row = []\n",
    "        for reg in range(regs):\n",
    "            gru_errs = []\n",
    "            lstm_errs = []\n",
    "            for ens_idx in range(ens):\n",
    "                gru_f_name = build_file_name(fold, reg, ens_idx, \"gru\", error_metric, \"test\")\n",
    "                lstm_f_name = build_file_name(fold, reg, ens_idx, \"lstm\", error_metric, \"test\")\n",
    "                gru_errs_ens = load_npz(gru_f_name)\n",
    "                lstm_errs_ens = load_npz(lstm_f_name)\n",
    "                gru_errs.append(gru_errs_ens)\n",
    "                lstm_errs.append(lstm_errs_ens)\n",
    "            avg_gru_errs = np.mean(gru_errs)\n",
    "            avg_lstm_errs = np.mean(lstm_errs)\n",
    "            gru_err_row.append(avg_gru_errs)\n",
    "            lstm_err_row.append(avg_lstm_errs)\n",
    "        gru_err_row.append(\"GRU\")\n",
    "        lstm_err_row.append(\"LSTM\")\n",
    "        err_table.append(gru_err_row)\n",
    "        err_table.append(lstm_err_row)\n",
    "    print(tabulate(err_table, tablefmt='latex'))\n",
    "\n",
    "#call the comparison table\n",
    "make_comparison_table(\"mae\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
